[
  {
    "id": "3a315bb5-e590-449f-abee-236200c70376",
    "content": "## Namespace Context Awareness\nUnless explicitly investigating a cluster-wide or cross-namespace issue (as in concurrent failures), always scope your Kubernetes commands to the provided `app_namespace`. This ensures you are querying the correct resources and prevents wasted tool calls in irrelevant parts of the cluster.\n**Action:** Always include `-n {app_namespace}` in your `kubectl` commands unless intentionally looking elsewhere.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T05:05:48.986091",
    "last_updated": "2025-11-30T05:05:48.986094",
    "metadata": {}
  },
  {
    "id": "5cbef9aa-cb5b-426e-b010-2aeaca855e76",
    "content": "## Clear Submission Protocol\nAfter a thorough diagnosis, whether you identify a fault or confirm the absence of one, it is critical to use the `submit` tool precisely. Do not continue to call tools once you have reached a definitive conclusion.\n-   **Submit 'Yes'**: If you have successfully identified one or more faults and determined their root cause(s).\n-   **Submit 'No'**: If, after a complete and diligent investigation, you find no incidents or faults within the application.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T05:59:08.884712",
    "last_updated": "2025-11-30T05:59:08.884716",
    "metadata": {}
  },
  {
    "id": "6a4ee865-4b9d-4030-8b0b-8925b39beb82",
    "content": "## Diagnosing Resource Exhaustion (CPU/Memory) - VERIFIED\nIf observed symptoms include performance degradation, slowness, unexpected pod restarts (e.g., OOMKilled), or high CPU alerts, resource requests and limits are a primary suspect. The ground truth analysis for `astronomy_shop_ad_service_high_cpu` explicitly confirms this diagnostic path.\n\n**Action:** Before delving into application logs or complex metrics, use `kubectl describe pod <pod_name> -n {app_namespace}` to inspect the CPU and memory requests and limits defined for containers within affected pods. Insufficient requests can lead to throttling, while low limits can result in OOMKills or CPU starvation. After checking limits, proceed to `kubectl logs` if issues persist.\n\n**Tools:**\n1.  `kubectl describe pod <pod_name> -n {app_namespace}`\n2.  `kubectl logs <pod_name> -n {app_namespace}`",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653032",
    "last_updated": "2025-11-30T06:48:12.653036",
    "metadata": {}
  },
  {
    "id": "2f88098e-15ba-465f-97ef-f207cf097168",
    "content": "## Investigating Storage-Related Faults\nWhen applications, especially stateful ones, fail to start, crash due to volume errors, or exhibit data consistency issues, storage configuration is a common culprit. This is evidenced by failures like `duplicate_pvc_mounts_hotel_reservation`.\n\n**Action:** If storage is involved, thoroughly examine PersistentVolumeClaims (PVCs) and their bound PersistentVolumes (PVs). Check the PVC's status, events, and description for any errors, pending states, or indications of incorrect binding. Also, review the events of affected pods for volume attachment failures or warnings about duplicate mounts.\n\n**Tools:**\n1.  `kubectl get pvc -n {app_namespace}`\n2.  `kubectl describe pvc <pvc_name> -n {app_namespace}`\n3.  `kubectl describe pod <pod_name> -n {app_namespace}` (to check pod events for volume-related errors).",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653391",
    "last_updated": "2025-11-30T06:48:12.653394",
    "metadata": {}
  },
  {
    "id": "127fc25a-8e89-4185-99ed-510372ba8098",
    "content": "## Verifying Application Configuration and Permissions\nIncorrect application configuration or insufficient Kubernetes API permissions are frequent causes of failures, often manifesting as connection errors, authentication failures, or unexpected application behavior. This is relevant to issues like `revoke_auth_mongodb-1` and `misconfig_app_hotel_res`.\n\n**Action:** If basic pod/deployment status appears healthy but the application is not functioning as expected, investigate the `ConfigMap`s and `Secret`s that the application consumes. Also, check the `ServiceAccount` used by the application's pods and its associated `RoleBindings`/`ClusterRoleBindings` if API access or authentication issues are suspected.\n\n**Tools:**\n1.  `kubectl get configmaps -n {app_namespace}`\n2.  `kubectl get secrets -n {app_namespace}`\n3.  `kubectl describe deployment <deployment_name> -n {app_namespace}` (to see which ConfigMaps/Secrets are mounted).\n4.  `kubectl describe serviceaccount <sa_name> -n {app_namespace}` (if permissions are suspected).",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653748",
    "last_updated": "2025-11-30T06:48:12.653751",
    "metadata": {}
  },
  {
    "id": "3083bdab-14f0-4a4e-9871-01ba23641f47",
    "content": "## Efficient Initial Diagnosis: Start with Pod State and Events\nWhen any service is suspected to have a fault, your absolute first step should be to thoroughly examine the state and recent events of its associated pods. Use `kubectl get pods -n {app_namespace}` to quickly identify unhealthy pods (e.g., `CrashLoopBackOff`, `Pending`, `Error`). For any such pod, immediately follow up with `kubectl describe pod <pod_name> -n {app_namespace}`. The 'Events' section of the `describe` output is a treasure trove of information, often revealing the root cause directly, such as image pull errors, OOMKilled containers, readiness/liveness probe failures, or volume mount issues. This initial investigation is quick and highly effective, often preventing the need for more complex or time-consuming diagnostic steps.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T07:55:52.649394",
    "last_updated": "2025-11-30T07:55:52.649396",
    "metadata": {}
  },
  {
    "id": "4e1c26c3-9213-4cff-8f60-66f301113924",
    "content": "## Persistent Storage Faults: Focus on PVCs and PVs\nWhen an application experiences issues related to data persistence, volume mounting, or storage access (e.g., data loss, 'Read-only file system' errors, pods failing to start due to volume errors, or `duplicate_pvc_mounts`), systematically investigate `PersistentVolumeClaims (PVCs)` and `PersistentVolumes (PVs)`. \n1.  **Check PVC Status**: Use `kubectl get pvc -n {app_namespace}` to list all PVCs. Then, `kubectl describe pvc <pvc_name> -n {app_namespace}` to examine its status (e.g., `Bound`, `Pending`), the `PersistentVolume` it's bound to, and any relevant events or warnings.\n2.  **Inspect PV Status**: If the PVC is bound, check the associated `PersistentVolume` using `kubectl describe pv <pv_name>` (PVs are cluster-scoped, so no namespace needed here) for any issues with the underlying storage provisioner.\n3.  **Pod Volume Events**: Always cross-reference with `kubectl describe pod <pod_name> -n {app_namespace}` to see events specifically related to volume attachment or mounting failures.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T07:55:52.651063",
    "last_updated": "2025-11-30T07:55:52.651066",
    "metadata": {}
  },
  {
    "id": "70f33ee2-7bed-450e-8b31-05f17da6a84a",
    "content": "## Authentication/Authorization: Investigate ServiceAccounts and RBAC\nIf an application reports 'Permission Denied', 'Forbidden', or 'Unauthorized' errors when interacting with Kubernetes API or other services, suspect issues with its `ServiceAccount` and `Role-Based Access Control (RBAC)` permissions.\n1.  **Identify ServiceAccount**: First, determine which `ServiceAccount` the problematic pod is using by checking its spec: `kubectl describe pod <pod_name> -n {app_namespace}`. Look for `Service Account:` or `serviceAccountName:`.\n2.  **Review RBAC Bindings**: Examine the `RoleBindings` and `ClusterRoleBindings` that grant permissions to this `ServiceAccount`. Use `kubectl get rolebinding -n {app_namespace}` and `kubectl get clusterrolebinding` to list them. Then, `kubectl describe rolebinding <name> -n {app_namespace}` or `kubectl describe clusterrolebinding <name>` to understand the specific `Roles` and `ClusterRoles` applied and the permissions they grant. Ensure the necessary API access is provided.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T07:55:52.651424",
    "last_updated": "2025-11-30T07:55:52.651427",
    "metadata": {}
  }
]