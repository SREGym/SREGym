[
  {
    "id": "06b55845-1d1d-4fcd-8776-385612faa3f4",
    "content": "## Initial Diagnostic Action: Always Start with Tools\nGiven the task to diagnose faults, your absolute first step should be to use available diagnostic tools to gather initial state and telemetry. Do not attempt to deduce without data. Begin by broadly surveying the application's health and resource status within its namespace. A common starting point is to use `kubectl get all -n {app_namespace}` to list all relevant resources, followed by `kubectl describe` for any resources that appear unhealthy or suspicious.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.391384",
    "last_updated": "2025-12-02T00:59:20.391387",
    "metadata": {}
  },
  {
    "id": "a6da1b14-e935-46a1-a91d-696d95f4692c",
    "content": "## Systematic Diagnostic Workflow for Efficiency\nTo reduce latency and attempts, adopt a systematic, hierarchical approach to diagnosis:\n1.  **Broad Overview:** Start with `kubectl get all -n <namespace>` to identify all running resources and their basic status.\n2.  **Detailed Status & Events:** For any suspicious or non-ready resources (e.g., pods in Pending/Error state, deployments not fully available), use `kubectl describe <resource_type>/<resource_name> -n <namespace>`. Pay close attention to the `Events` section and `Status` conditions, as these often reveal immediate issues like image pull errors, insufficient resources, or failed probes.\n3.  **Logs for Running Pods:** If a pod is running but misbehaving or showing application-level errors, check its logs with `kubectl logs <pod_name> -n <namespace> -c <container_name>` (specify container if multiple).\n4.  **Configuration & Dependencies:** If the above do not yield a root cause, investigate related configurations (`ConfigMap`, `Secret`), networking (`Service`, `Ingress`, `NetworkPolicy`), and storage (`PVC`, `PV`) resources that the faulty component depends on.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.391566",
    "last_updated": "2025-12-02T00:59:20.391568",
    "metadata": {}
  },
  {
    "id": "7e17d3e9-9f3c-4c17-a14c-268c2477aedb",
    "content": "## Specific Check: Network Policy Related Failures\nIf network communication issues are suspected (e.g., services cannot reach each other, timeouts, connection refused), always check for `NetworkPolicy` resources. Use `kubectl get networkpolicies -n <namespace>` to list them, and then `kubectl describe networkpolicy <policy_name> -n <namespace>` to understand their rules. Misconfigured network policies are a common cause of connectivity problems.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.391772",
    "last_updated": "2025-12-02T00:59:20.391775",
    "metadata": {}
  },
  {
    "id": "1a043531-0bb2-45b8-8646-890341ac4d75",
    "content": "## Specific Check: Resource Limit Related Failures (CPU/Memory)\nFor resource-related issues such as high CPU usage, OOMKilled (Out Of Memory Killed) pods, or pods failing to schedule due to resource constraints, prioritize inspecting the pod's resource requests and limits. Use `kubectl describe pod <pod_name> -n <namespace>` and examine the 'Limits' and 'Requests' sections. Incorrectly set resource limits or requests are a frequent root cause.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.391999",
    "last_updated": "2025-12-02T00:59:20.392002",
    "metadata": {}
  },
  {
    "id": "d5251ec4-18f4-48af-80c2-c453da554a0d",
    "content": "## Strategy for Concurrent or Multi-Service Failures\nWhen multiple services or applications exhibit concurrent failures, do not treat them as isolated incidents immediately. Instead, investigate each affected service and its namespace independently *while also* considering shared underlying infrastructure components (e.g., a shared database, ingress controller, network policies affecting multiple applications, or cluster-wide resource exhaustion). Look for a single point of failure that could impact all observed symptoms.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.392261",
    "last_updated": "2025-12-02T00:59:20.392264",
    "metadata": {}
  },
  {
    "id": "2bd56218-9a6d-4b32-adf9-61839ef0b28b",
    "content": "## Clarity on Root Cause Identification and Stopping Criteria\nA fault is truly localized when you can clearly articulate the *specific component* (e.g., a particular pod, deployment, network policy, or configuration) and the *exact reason* (e.g., misconfigured resource limit, incorrect network rule, application error in logs, out-of-date image) for its failure. Do not stop at identifying a symptom; continue until you understand the underlying mechanism of the failure. Once this precise root cause is identified, and you can explain *why* it failed, you should submit your findings.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-12-02T00:59:20.392531",
    "last_updated": "2025-12-02T00:59:20.392534",
    "metadata": {}
  }
]