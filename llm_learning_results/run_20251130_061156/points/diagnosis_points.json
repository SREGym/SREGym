[
  {
    "id": "3a315bb5-e590-449f-abee-236200c70376",
    "content": "## Namespace Context Awareness\nUnless explicitly investigating a cluster-wide or cross-namespace issue (as in concurrent failures), always scope your Kubernetes commands to the provided `app_namespace`. This ensures you are querying the correct resources and prevents wasted tool calls in irrelevant parts of the cluster.\n**Action:** Always include `-n {app_namespace}` in your `kubectl` commands unless intentionally looking elsewhere.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T05:05:48.986091",
    "last_updated": "2025-11-30T05:05:48.986094",
    "metadata": {}
  },
  {
    "id": "5cbef9aa-cb5b-426e-b010-2aeaca855e76",
    "content": "## Clear Submission Protocol\nAfter a thorough diagnosis, whether you identify a fault or confirm the absence of one, it is critical to use the `submit` tool precisely. Do not continue to call tools once you have reached a definitive conclusion.\n-   **Submit 'Yes'**: If you have successfully identified one or more faults and determined their root cause(s).\n-   **Submit 'No'**: If, after a complete and diligent investigation, you find no incidents or faults within the application.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T05:59:08.884712",
    "last_updated": "2025-11-30T05:59:08.884716",
    "metadata": {}
  },
  {
    "id": "6a4ee865-4b9d-4030-8b0b-8925b39beb82",
    "content": "## Diagnosing Resource Exhaustion (CPU/Memory) - VERIFIED\nIf observed symptoms include performance degradation, slowness, unexpected pod restarts (e.g., OOMKilled), or high CPU alerts, resource requests and limits are a primary suspect. The ground truth analysis for `astronomy_shop_ad_service_high_cpu` explicitly confirms this diagnostic path.\n\n**Action:** Before delving into application logs or complex metrics, use `kubectl describe pod <pod_name> -n {app_namespace}` to inspect the CPU and memory requests and limits defined for containers within affected pods. Insufficient requests can lead to throttling, while low limits can result in OOMKills or CPU starvation. After checking limits, proceed to `kubectl logs` if issues persist.\n\n**Tools:**\n1.  `kubectl describe pod <pod_name> -n {app_namespace}`\n2.  `kubectl logs <pod_name> -n {app_namespace}`",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653032",
    "last_updated": "2025-11-30T06:48:12.653036",
    "metadata": {}
  },
  {
    "id": "2f88098e-15ba-465f-97ef-f207cf097168",
    "content": "## Investigating Storage-Related Faults\nWhen applications, especially stateful ones, fail to start, crash due to volume errors, or exhibit data consistency issues, storage configuration is a common culprit. This is evidenced by failures like `duplicate_pvc_mounts_hotel_reservation`.\n\n**Action:** If storage is involved, thoroughly examine PersistentVolumeClaims (PVCs) and their bound PersistentVolumes (PVs). Check the PVC's status, events, and description for any errors, pending states, or indications of incorrect binding. Also, review the events of affected pods for volume attachment failures or warnings about duplicate mounts.\n\n**Tools:**\n1.  `kubectl get pvc -n {app_namespace}`\n2.  `kubectl describe pvc <pvc_name> -n {app_namespace}`\n3.  `kubectl describe pod <pod_name> -n {app_namespace}` (to check pod events for volume-related errors).",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653391",
    "last_updated": "2025-11-30T06:48:12.653394",
    "metadata": {}
  },
  {
    "id": "127fc25a-8e89-4185-99ed-510372ba8098",
    "content": "## Verifying Application Configuration and Permissions\nIncorrect application configuration or insufficient Kubernetes API permissions are frequent causes of failures, often manifesting as connection errors, authentication failures, or unexpected application behavior. This is relevant to issues like `revoke_auth_mongodb-1` and `misconfig_app_hotel_res`.\n\n**Action:** If basic pod/deployment status appears healthy but the application is not functioning as expected, investigate the `ConfigMap`s and `Secret`s that the application consumes. Also, check the `ServiceAccount` used by the application's pods and its associated `RoleBindings`/`ClusterRoleBindings` if API access or authentication issues are suspected.\n\n**Tools:**\n1.  `kubectl get configmaps -n {app_namespace}`\n2.  `kubectl get secrets -n {app_namespace}`\n3.  `kubectl describe deployment <deployment_name> -n {app_namespace}` (to see which ConfigMaps/Secrets are mounted).\n4.  `kubectl describe serviceaccount <sa_name> -n {app_namespace}` (if permissions are suspected).",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:48:12.653748",
    "last_updated": "2025-11-30T06:48:12.653751",
    "metadata": {}
  }
]