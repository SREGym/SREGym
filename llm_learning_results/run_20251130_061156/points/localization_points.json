[
  {
    "id": "ae954b7a-1933-44d2-8bd9-c26db3b92fe5",
    "content": "## Resource Contention and Limits Troubleshooting\nIf performance degradation (e.g., high CPU, high memory usage, or unexpected restarts like OOMKills) is observed or suspected, prioritize checking the resource requests and limits defined for the affected pods. Use `kubectl describe pod <pod-name> -n <namespace>` to inspect the `Limits` and `Requests` sections. This can quickly reveal if the application is being throttled or starved of resources, as seen in `astronomy_shop_ad_service_high_cpu` and `valkey_memory_disruption` failures. Address resource misconfigurations before delving into application-specific metrics.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:02:21.061061",
    "last_updated": "2025-11-30T06:02:21.061064",
    "metadata": {}
  },
  {
    "id": "ea2bc856-513c-44ca-ba0e-5e342eafc8aa",
    "content": "## Systematic Configuration, Authorization, and Storage Review\nWhen application misbehavior, denied access, or data persistence issues are identified (e.g., `misconfig_app_hotel_res`, `revoke_auth_mongodb-1`, `duplicate_pvc_mounts_hotel_reservation`), systematically review relevant configuration, authorization, and storage resources:\n- **Application Configuration:** Inspect `ConfigMap` and `Secret` resources linked to the failing pods/deployments. Use `kubectl get configmap <name> -o yaml` and `kubectl get secret <name> -o yaml` (be cautious with secret content, focus on names and keys).\n- **Authorization Failures:** For permission-related errors, examine `ServiceAccount`s used by pods, along with `Role`s, `RoleBinding`s, `ClusterRole`s, and `ClusterRoleBinding`s. Start by checking the `serviceAccountName` in the pod definition.\n- **Storage Issues:** For data access or persistence problems, investigate `PersistentVolumeClaim` (PVC) and `PersistentVolume` (PV) resources associated with the application. Verify their status, correct mounting in pod definitions, and associated `StorageClass`.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:02:21.061596",
    "last_updated": "2025-11-30T06:02:21.061599",
    "metadata": {}
  },
  {
    "id": "4090dee3-7a74-4249-97b0-a3fc7fe0df80",
    "content": "## Diagnosing Network Policy Related Failures\nIf connectivity issues are suspected (e.g., services unable to communicate, timeouts, or specific traffic blocks), systematically investigate `NetworkPolicy` resources. Use `kubectl get networkpolicies -n <namespace>` to list policies and then `kubectl describe networkpolicy <policy-name> -n <namespace>` to understand their rules. Misconfigured or overly restrictive NetworkPolicies are a common cause of unexpected communication failures, as seen in `network_policy_block` scenarios. This check should be prioritized when network-related symptoms are observed.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:55:18.764645",
    "last_updated": "2025-11-30T06:55:18.764648",
    "metadata": {}
  },
  {
    "id": "283f08b8-a00d-48d3-8d73-acb6a92882c1",
    "content": "## Efficiency in Root Cause Submission\nOnce a clear and actionable root cause for the fault has been identified and verified through testing, **immediately proceed to submit your findings**. Do not perform additional exploratory tool calls or gather further non-essential information. The objective is to localize the fault accurately and efficiently. Prolonged diagnosis after root cause identification increases latency and unnecessary tool call attempts, negatively impacting performance metrics. Focus on delivering the root cause description concisely and directly.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T06:55:18.765272",
    "last_updated": "2025-11-30T06:55:18.765275",
    "metadata": {}
  }
]