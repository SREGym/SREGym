[
  {
    "id": "998d0e2e-961b-46a1-9f0e-e5fca8431f82",
    "content": "## Initial Diagnostic Steps\nTo begin your diagnosis, always start by performing a broad overview of the application's state. Your first tool calls should be to get a comprehensive list of running pods and deployments within the target namespace. This helps you understand the current service landscape and identify any immediate issues like pending pods or crash loops.\n\n**Actionable First Steps:**\n1.  **`kubectl get pods -n {app_namespace}`**: Identify all pods, their statuses (Running, Pending, CrashLoopBackOff, Error, OOMKilled), and restart counts.\n2.  **`kubectl get deployments -n {app_namespace}`**: Understand how applications are deployed and their desired vs. current state.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T22:36:12.374189",
    "last_updated": "2025-11-30T22:36:12.374191",
    "metadata": {}
  },
  {
    "id": "289f370e-02e4-4187-a3d1-f910c617c1ee",
    "content": "## Systematic Troubleshooting Workflow\nOnce you have an overview, follow a systematic approach to narrow down the fault. Prioritize checks that provide the most immediate diagnostic information, then progressively dig deeper based on symptoms.\n\n**Recommended Flow:**\n1.  **Pod Status & Events**: If `kubectl get pods` shows issues (e.g., `CrashLoopBackOff`, `Pending`, `Error`), immediately investigate specific pods:\n    *   **`kubectl describe pod <pod_name> -n {app_namespace}`**: Check events, resource requests/limits, volume mounts, and container status. This is crucial for CPU/memory issues (as seen in `astronomy_shop_ad_service_high_cpu`) and PVC mount problems (`duplicate_pvc_mounts_hotel_reservation`).\n    *   **`kubectl logs <pod_name> -n {app_namespace}`**: Retrieve application logs for detailed error messages.\n    *   **`kubectl get events -n {app_namespace}`**: Look for cluster-wide or namespace-specific events that might indicate underlying infrastructure problems.\n2.  **Resource Utilization & Configuration**: If pods are running but exhibiting performance issues or unexpected behavior:\n    *   **Check resource limits/requests**: Verify that pods have appropriate CPU and memory limits. Low limits can lead to `OOMKilled` or throttling (e.g., `valkey_memory_disruption`, `astronomy_shop_ad_service_high_cpu`).\n    *   **Inspect ConfigMaps and Secrets**: If application behavior is incorrect, check relevant `ConfigMap`s and `Secret`s for misconfigurations (e.g., `misconfig_app_hotel_res`).\n3.  **Networking & Connectivity**: If services are unreachable or experiencing timeouts:\n    *   **`kubectl get services -n {app_namespace}`**: Verify service endpoints and types.\n    *   **`kubectl get ingresses -n {app_namespace}`**: Check external access rules.\n    *   **`kubectl get networkpolicies -n {app_namespace}`**: Crucially, review `NetworkPolicy` resources if connectivity issues persist (as seen in `network_policy_block`). Explicitly check if any policies are blocking traffic to or from the affected pods/services.\n4.  **Storage & Permissions**: If data persistence or access is an issue:\n    *   **`kubectl get pvc -n {app_namespace}`** and **`kubectl get pv`**: Check the status and binding of PersistentVolumeClaims and PersistentVolumes.\n    *   **`kubectl get serviceaccounts -n {app_namespace}`**, **`kubectl get roles -n {app_namespace}`**, **`kubectl get rolebindings -n {app_namespace}`**: Investigate RBAC issues if applications lack necessary permissions (e.g., `revoke_auth_mongodb-1`).\n\nFor concurrent failures affecting multiple services or namespaces (e.g., `social_net_hotel_res_astro_shop_concurrent_failures`), apply this systematic workflow to each affected component independently until the common root cause is found.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T22:36:12.374378",
    "last_updated": "2025-11-30T22:36:12.374381",
    "metadata": {}
  },
  {
    "id": "040bd449-15e8-4a7f-8afb-6f6f7f0d8d9a",
    "content": "## Focus on the Root Cause, Not Just the Symptom\nWhen localizing faults, your goal is to identify the *root cause*, not just the immediate symptom. For example, if a pod is in `CrashLoopBackOff`, the symptom is the crashing pod. The root cause might be an `OOMKilled` event due to insufficient memory limits (requiring a `kubectl describe pod` to find), or a misconfigured environment variable (requiring `kubectl describe pod` and potentially `kubectl get configmap`). Always ask 'Why did this happen?' and use tools to answer that question, going as deep as necessary until the fundamental problem is uncovered.",
    "source": "learned",
    "category": "tool_usage",
    "priority": 6,
    "verified": false,
    "verification_count": 0,
    "success_count": 0,
    "failure_count": 0,
    "conflicts_with": [],
    "replaces": null,
    "replaced_by": null,
    "active": true,
    "created_at": "2025-11-30T22:36:12.374599",
    "last_updated": "2025-11-30T22:36:12.374601",
    "metadata": {}
  }
]